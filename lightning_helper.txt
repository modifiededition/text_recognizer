usage: run_experiment.py [--logger [LOGGER]]
                         [--checkpoint_callback [CHECKPOINT_CALLBACK]]
                         [--enable_checkpointing [ENABLE_CHECKPOINTING]]
                         [--default_root_dir DEFAULT_ROOT_DIR]
                         [--gradient_clip_val GRADIENT_CLIP_VAL]
                         [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]
                         [--process_position PROCESS_POSITION]
                         [--num_nodes NUM_NODES]
                         [--num_processes NUM_PROCESSES] [--devices DEVICES]
                         [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]
                         [--tpu_cores TPU_CORES] [--ipus IPUS]
                         [--log_gpu_memory LOG_GPU_MEMORY]
                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]
                         [--enable_progress_bar [ENABLE_PROGRESS_BAR]]
                         [--overfit_batches OVERFIT_BATCHES]
                         [--track_grad_norm TRACK_GRAD_NORM]
                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]
                         [--fast_dev_run [FAST_DEV_RUN]]
                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]
                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]
                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]
                         [--max_time MAX_TIME]
                         [--limit_train_batches LIMIT_TRAIN_BATCHES]
                         [--limit_val_batches LIMIT_VAL_BATCHES]
                         [--limit_test_batches LIMIT_TEST_BATCHES]
                         [--limit_predict_batches LIMIT_PREDICT_BATCHES]
                         [--val_check_interval VAL_CHECK_INTERVAL]
                         [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]
                         [--log_every_n_steps LOG_EVERY_N_STEPS]
                         [--accelerator ACCELERATOR] [--strategy STRATEGY]
                         [--sync_batchnorm [SYNC_BATCHNORM]]
                         [--precision PRECISION]
                         [--enable_model_summary [ENABLE_MODEL_SUMMARY]]
                         [--weights_summary WEIGHTS_SUMMARY]
                         [--weights_save_path WEIGHTS_SAVE_PATH]
                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]
                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                         [--profiler PROFILER] [--benchmark [BENCHMARK]]
                         [--deterministic [DETERMINISTIC]]
                         [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]
                         [--auto_lr_find [AUTO_LR_FIND]]
                         [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]
                         [--detect_anomaly [DETECT_ANOMALY]]
                         [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]
                         [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]
                         [--plugins PLUGINS] [--amp_backend AMP_BACKEND]
                         [--amp_level AMP_LEVEL]
                         [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]
                         [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]
                         [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]
                         [--terminate_on_nan [TERMINATE_ON_NAN]] [--wandb]
                         [--data_class DATA_CLASS] [--model_class MODEL_CLASS]
                         [--load_checkpoint LOAD_CHECKPOINT]
                         [--stop_early STOP_EARLY] [--batch_size BATCH_SIZE]
                         [--num_workers NUM_WORKERS]
                         [--augment_data AUGMENT_DATA] [--fc1 FC1] [--fc2 FC2]
                         [--fc_dropout FC_DROPOUT] [--optimizer OPTIMIZER]
                         [--lr LR] [--one_cycle_max_lr ONE_CYCLE_MAX_LR]
                         [--one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS]
                         [--loss LOSS] [--help]

options:
  --wandb               If passed, logs experiment results to Weights &
                        Biases. Otherwise logs only to local Tensorboard.
  --data_class DATA_CLASS
                        String identifier for the data class, relative to
                        text_recognizer.data.
  --model_class MODEL_CLASS
                        String identifier for the model class, relative to
                        text_recognizer.models.
  --load_checkpoint LOAD_CHECKPOINT
                        If passed, loads a model from the provided path.
  --stop_early STOP_EARLY
                        If non-zero, applies early stopping, with the provided
                        value as the 'patience' argument. Default is 0.
  --help, -h

pl.Trainer:
  --logger [LOGGER]     Logger (or iterable collection of loggers) for
                        experiment tracking. A ``True`` value uses the default
                        ``TensorBoardLogger``. ``False`` will disable logging.
                        If multiple loggers are provided and the `save_dir`
                        property of that logger is not set, local files
                        (checkpoints, profiler traces, etc.) are saved in
                        ``default_root_dir`` rather than in the ``log_dir`` of
                        any of the individual loggers. Default: ``True``.
  --checkpoint_callback [CHECKPOINT_CALLBACK]
                        If ``True``, enable checkpointing. Default: ``None``.
                        .. deprecated:: v1.5 ``checkpoint_callback`` has been
                        deprecated in v1.5 and will be removed in v1.7. Please
                        consider using ``enable_checkpointing`` instead.
  --enable_checkpointing [ENABLE_CHECKPOINTING]
                        If ``True``, enable checkpointing. It will configure a
                        default ModelCheckpoint callback if there is no user-
                        defined ModelCheckpoint in :paramref:`~pytorch_lightni
                        ng.trainer.trainer.Trainer.callbacks`. Default:
                        ``True``.
  --default_root_dir DEFAULT_ROOT_DIR
                        Default path for logs and weights when no
                        logger/ckpt_callback passed. Default: ``os.getcwd()``.
                        Can be remote file paths such as `s3://mybucket/path`
                        or 'hdfs://path/'
  --gradient_clip_val GRADIENT_CLIP_VAL
                        The value at which to clip gradients. Passing
                        ``gradient_clip_val=None`` disables gradient clipping.
                        If using Automatic Mixed Precision (AMP), the
                        gradients will be unscaled before. Default: ``None``.
  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM
                        The gradient clipping algorithm to use. Pass
                        ``gradient_clip_algorithm="value"`` to clip by value,
                        and ``gradient_clip_algorithm="norm"`` to clip by
                        norm. By default it will be set to ``"norm"``.
  --process_position PROCESS_POSITION
                        Orders the progress bar when running multiple models
                        on same machine. .. deprecated:: v1.5
                        ``process_position`` has been deprecated in v1.5 and
                        will be removed in v1.7. Please pass :class:`~pytorch_
                        lightning.callbacks.progress.TQDMProgressBar` with
                        ``process_position`` directly to the Trainer's
                        ``callbacks`` argument instead.
  --num_nodes NUM_NODES
                        Number of GPU nodes for distributed training. Default:
                        ``1``.
  --num_processes NUM_PROCESSES
                        Number of processes for distributed training with
                        ``accelerator="cpu"``. Default: ``1``.
  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,
                        `num_processes` or `ipus`, based on the accelerator
                        type.
  --gpus GPUS           Number of GPUs to train on (int) or which GPUs to
                        train on (list or str) applied per node Default:
                        ``None``.
  --auto_select_gpus [AUTO_SELECT_GPUS]
                        If enabled and ``gpus`` or ``devices`` is an integer,
                        pick available gpus automatically. This is especially
                        useful when GPUs are configured to be in "exclusive
                        mode", such that only one process at a time can access
                        them. Default: ``False``.
  --tpu_cores TPU_CORES
                        How many TPU cores to train on (1 or 8) / Single TPU
                        to train on (1) Default: ``None``.
  --ipus IPUS           How many IPUs to train on. Default: ``None``.
  --log_gpu_memory LOG_GPU_MEMORY
                        None, 'min_max', 'all'. Might slow performance. ..
                        deprecated:: v1.5 Deprecated in v1.5.0 and will be
                        removed in v1.7.0 Please use the
                        ``DeviceStatsMonitor`` callback directly instead.
  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE
                        How often to refresh progress bar (in steps). Value
                        ``0`` disables progress bar. Ignored when a custom
                        progress bar is passed to
                        :paramref:`~Trainer.callbacks`. Default: None, means a
                        suitable value will be chosen based on the environment
                        (terminal, Google COLAB, etc.). .. deprecated:: v1.5
                        ``progress_bar_refresh_rate`` has been deprecated in
                        v1.5 and will be removed in v1.7. Please pass :class:`
                        ~pytorch_lightning.callbacks.progress.TQDMProgressBar`
                        with ``refresh_rate`` directly to the Trainer's
                        ``callbacks`` argument instead. To disable the
                        progress bar, pass ``enable_progress_bar = False`` to
                        the Trainer.
  --enable_progress_bar [ENABLE_PROGRESS_BAR]
                        Whether to enable to progress bar by default. Default:
                        ``False``.
  --overfit_batches OVERFIT_BATCHES
                        Overfit a fraction of training data (float) or a set
                        number of batches (int). Default: ``0.0``.
  --track_grad_norm TRACK_GRAD_NORM
                        -1 no tracking. Otherwise tracks that p-norm. May be
                        set to 'inf' infinity-norm. If using Automatic Mixed
                        Precision (AMP), the gradients will be unscaled before
                        logging them. Default: ``-1``.
  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH
                        Check val every n train epochs. Default: ``1``.
  --fast_dev_run [FAST_DEV_RUN]
                        Runs n if set to ``n`` (int) else 1 if set to ``True``
                        batch(es) of train, val and test to find any bugs (ie:
                        a sort of unit test). Default: ``False``.
  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES
                        Accumulates grads every k batches or as set up in the
                        dict. Default: ``None``.
  --max_epochs MAX_EPOCHS
                        Stop training once this number of epochs is reached.
                        Disabled by default (None). If both max_epochs and
                        max_steps are not specified, defaults to ``max_epochs
                        = 1000``. To enable infinite training, set
                        ``max_epochs = -1``.
  --min_epochs MIN_EPOCHS
                        Force training for at least these many epochs.
                        Disabled by default (None).
  --max_steps MAX_STEPS
                        Stop training after this number of steps. Disabled by
                        default (-1). If ``max_steps = -1`` and ``max_epochs =
                        None``, will default to ``max_epochs = 1000``. To
                        enable infinite training, set ``max_epochs`` to
                        ``-1``.
  --min_steps MIN_STEPS
                        Force training for at least these number of steps.
                        Disabled by default (``None``).
  --max_time MAX_TIME   Stop training after this amount of time has passed.
                        Disabled by default (``None``). The time duration can
                        be specified in the format DD:HH:MM:SS (days, hours,
                        minutes seconds), as a :class:`datetime.timedelta`, or
                        a dictionary with keys that will be passed to
                        :class:`datetime.timedelta`.
  --limit_train_batches LIMIT_TRAIN_BATCHES
                        How much of training dataset to check (float =
                        fraction, int = num_batches). Default: ``1.0``.
  --limit_val_batches LIMIT_VAL_BATCHES
                        How much of validation dataset to check (float =
                        fraction, int = num_batches). Default: ``1.0``.
  --limit_test_batches LIMIT_TEST_BATCHES
                        How much of test dataset to check (float = fraction,
                        int = num_batches). Default: ``1.0``.
  --limit_predict_batches LIMIT_PREDICT_BATCHES
                        How much of prediction dataset to check (float =
                        fraction, int = num_batches). Default: ``1.0``.
  --val_check_interval VAL_CHECK_INTERVAL
                        How often to check the validation set. Pass a
                        ``float`` in the range [0.0, 1.0] to check after a
                        fraction of the training epoch. Pass an ``int`` to
                        check after a fixed number of training batches.
                        Default: ``1.0``.
  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS
                        How often to flush logs to disk (defaults to every 100
                        steps). .. deprecated:: v1.5
                        ``flush_logs_every_n_steps`` has been deprecated in
                        v1.5 and will be removed in v1.7. Please configure
                        flushing directly in the logger instead.
  --log_every_n_steps LOG_EVERY_N_STEPS
                        How often to log within steps. Default: ``50``.
  --accelerator ACCELERATOR
                        Supports passing different accelerator types ("cpu",
                        "gpu", "tpu", "ipu", "hpu", "auto") as well as custom
                        accelerator instances. .. deprecated:: v1.5 Passing
                        training strategies (e.g., 'ddp') to ``accelerator``
                        has been deprecated in v1.5.0 and will be removed in
                        v1.7.0. Please use the ``strategy`` argument instead.
  --strategy STRATEGY   Supports different training strategies with aliases as
                        well custom strategies. Default: ``None``.
  --sync_batchnorm [SYNC_BATCHNORM]
                        Synchronize batch norm layers between process
                        groups/whole world. Default: ``False``.
  --precision PRECISION
                        Double precision (64), full precision (32), half
                        precision (16) or bfloat16 precision (bf16). Can be
                        used on CPU, GPU, TPUs, HPUs or IPUs. Default: ``32``.
  --enable_model_summary [ENABLE_MODEL_SUMMARY]
                        Whether to enable model summarization by default.
                        Default: ``True``.
  --weights_summary WEIGHTS_SUMMARY
                        Prints a summary of the weights when training begins.
                        .. deprecated:: v1.5 ``weights_summary`` has been
                        deprecated in v1.5 and will be removed in v1.7. To
                        disable the summary, pass ``enable_model_summary =
                        False`` to the Trainer. To customize the summary, pass
                        :class:`~pytorch_lightning.callbacks.model_summary.Mod
                        elSummary` directly to the Trainer's ``callbacks``
                        argument.
  --weights_save_path WEIGHTS_SAVE_PATH
                        Where to save weights if specified. Will override
                        default_root_dir for checkpoints only. Use this if for
                        whatever reason you need the checkpoints stored in a
                        different place than the logs written in
                        `default_root_dir`. Can be remote file paths such as
                        `s3://mybucket/path` or 'hdfs://path/' Defaults to
                        `default_root_dir`. .. deprecated:: v1.6
                        ``weights_save_path`` has been deprecated in v1.6 and
                        will be removed in v1.8. Please pass ``dirpath``
                        directly to the :class:`~pytorch_lightning.callbacks.m
                        odel_checkpoint.ModelCheckpoint` callback.
  --num_sanity_val_steps NUM_SANITY_VAL_STEPS
                        Sanity check runs n validation batches before starting
                        the training routine. Set it to `-1` to run all
                        batches in all validation dataloaders. Default: ``2``.
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        Path/URL of the checkpoint from which training is
                        resumed. If there is no checkpoint file at the path,
                        an exception is raised. If resuming from mid-epoch
                        checkpoint, training will start from the beginning of
                        the next epoch. .. deprecated:: v1.5
                        ``resume_from_checkpoint`` is deprecated in v1.5 and
                        will be removed in v2.0. Please pass the path to
                        ``Trainer.fit(..., ckpt_path=...)`` instead.
  --profiler PROFILER   To profile individual steps during training and assist
                        in identifying bottlenecks. Default: ``None``.
  --benchmark [BENCHMARK]
                        Sets ``torch.backends.cudnn.benchmark``. Defaults to
                        ``True`` if :paramref:`~pytorch_lightning.trainer.trai
                        ner.Trainer.deterministic` is ``False``. Overwrite to
                        manually set a different value. Default: ``None``.
  --deterministic [DETERMINISTIC]
                        If ``True``, sets whether PyTorch operations must use
                        deterministic algorithms. Default: ``False``.
  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS
                        Set to a non-negative integer to reload dataloaders
                        every n epochs. Default: ``0``.
  --auto_lr_find [AUTO_LR_FIND]
                        If set to True, will make trainer.tune() run a
                        learning rate finder, trying to optimize initial
                        learning for faster convergence. trainer.tune() method
                        will set the suggested learning rate in self.lr or
                        self.learning_rate in the LightningModule. To use a
                        different key set a string instead of True with the
                        key name. Default: ``False``.
  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]
                        Explicitly enables or disables sampler replacement. If
                        not specified this will toggled automatically when DDP
                        is used. By default it will add ``shuffle=True`` for
                        train sampler and ``shuffle=False`` for val/test
                        sampler. If you want to customize it, you can set
                        ``replace_sampler_ddp=False`` and add your own
                        distributed sampler.
  --detect_anomaly [DETECT_ANOMALY]
                        Enable anomaly detection for the autograd engine.
                        Default: ``False``.
  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]
                        If set to True, will `initially` run a batch size
                        finder trying to find the largest batch size that fits
                        into memory. The result will be stored in
                        self.batch_size in the LightningModule. Additionally,
                        can be set to either `power` that estimates the batch
                        size through a power search or `binsearch` that
                        estimates the batch size through a binary search.
                        Default: ``False``.
  --prepare_data_per_node [PREPARE_DATA_PER_NODE]
                        If True, each LOCAL_RANK=0 will call prepare data.
                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare
                        data .. deprecated:: v1.5 Deprecated in v1.5.0 and
                        will be removed in v1.7.0 Please set
                        ``prepare_data_per_node`` in ``LightningDataModule``
                        and/or ``LightningModule`` directly instead.
  --plugins PLUGINS     Plugins allow modification of core behavior like ddp
                        and amp, and enable custom lightning plugins. Default:
                        ``None``.
  --amp_backend AMP_BACKEND
                        The mixed precision backend to use ("native" or
                        "apex"). Default: ``'native''``.
  --amp_level AMP_LEVEL
                        The optimization level to use (O1, O2, etc...). By
                        default it will be set to "O2" if ``amp_backend`` is
                        set to "apex".
  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]
                        Whether to force internal logged metrics to be moved
                        to cpu. This can save some gpu memory, but can make
                        training slower. Use with attention. Default:
                        ``False``.
  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE
                        How to loop over the datasets when there are multiple
                        train loaders. In 'max_size_cycle' mode, the trainer
                        ends one epoch when the largest dataset is traversed,
                        and smaller datasets reload when running out of their
                        data. In 'min_size' mode, all the datasets reload when
                        reaching the minimum length of datasets. Default:
                        ``"max_size_cycle"``.
  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]
                        Whether to use `Stochastic Weight Averaging (SWA)
                        <https://pytorch.org/blog/pytorch-1.6-now-includes-
                        stochastic-weight-averaging/>`_. Default: ``False``.
                        .. deprecated:: v1.5 ``stochastic_weight_avg`` has
                        been deprecated in v1.5 and will be removed in v1.7.
                        Please pass :class:`~pytorch_lightning.callbacks.stoch
                        astic_weight_avg.StochasticWeightAveraging` directly
                        to the Trainer's ``callbacks`` argument instead.
  --terminate_on_nan [TERMINATE_ON_NAN]
                        If set to True, will terminate training (by raising a
                        `ValueError`) at the end of each training batch, if
                        any of the parameters or the loss are NaN or +/-inf.
                        .. deprecated:: v1.5 Trainer argument
                        ``terminate_on_nan`` was deprecated in v1.5 and will
                        be removed in 1.7. Please use ``detect_anomaly``
                        instead.

Data Args:
  --batch_size BATCH_SIZE
                        Number of examples to operate on per forward step.
                        Default is 128.
  --num_workers NUM_WORKERS
                        Number of additional processes to load data. Default
                        is 8.
  --augment_data AUGMENT_DATA

Model Args:
  --fc1 FC1
  --fc2 FC2
  --fc_dropout FC_DROPOUT

LitModel Args:
  --optimizer OPTIMIZER
                        optimizer class from torch.optim
  --lr LR
  --one_cycle_max_lr ONE_CYCLE_MAX_LR
  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS
  --loss LOSS           loss function from torch.nn.functional
